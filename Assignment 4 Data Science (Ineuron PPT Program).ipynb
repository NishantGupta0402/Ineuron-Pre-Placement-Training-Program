{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f15496b5",
   "metadata": {},
   "source": [
    "## General Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d6c68",
   "metadata": {},
   "source": [
    "### Sol 1 What is the purpose of the General Linear Model (GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3986f0d0",
   "metadata": {},
   "source": [
    "Ans Purpose of GLM is to establish a linear relation between independent and Predictor variable/s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca3b230",
   "metadata": {},
   "source": [
    "### Sol 2 What are the key assumptions of the General Linear Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161159e7",
   "metadata": {},
   "source": [
    "\n",
    "1. The relation ship between predictor and Varaiable is linear \n",
    "2. Errors are random and follow normal Distribution\n",
    "3. The variable are homosedastic\n",
    "4. The indpeendent variables are independent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed079308",
   "metadata": {},
   "source": [
    "### Sol 3 How do you interpret the coefficients in a GLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d24e3f",
   "metadata": {},
   "source": [
    "coefficiemts denote the unit change in variables causing the effect on predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e71b6",
   "metadata": {},
   "source": [
    "### Sol 4 What is the difference between a univariate and multivariate GLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcae049",
   "metadata": {},
   "source": [
    "Univariate denotes single independent variable \n",
    "Multivariate denotes more than variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e33bb8",
   "metadata": {},
   "source": [
    "### Sol 5 Explain the concept of interaction effects in a GLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96367cf3",
   "metadata": {},
   "source": [
    "Sometimes the relation between predictor and indepent variable can be represented by the interactive terms example \n",
    "multipication of two independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff6f50e",
   "metadata": {},
   "source": [
    "### Sol 6 How do you handle categorical predictors in a GLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7298254",
   "metadata": {},
   "source": [
    "There are various ways \n",
    "\n",
    "1. One Hot Encoding \n",
    "2. Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd72527",
   "metadata": {},
   "source": [
    "### Sol 7 What is the purpose of the design matrix in a GLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91956ea9",
   "metadata": {},
   "source": [
    "A colection of feature vectors for each "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4a8aa9",
   "metadata": {},
   "source": [
    "### Sol 8 How do you test the significance of predictors in a GLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af30e716",
   "metadata": {},
   "source": [
    "Sol Ftest,metrics Like RScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af04cda",
   "metadata": {},
   "source": [
    "### Sol 9 What is the difference between Type I, Type II, and Type III sums of squares in a GLM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e545912d",
   "metadata": {},
   "source": [
    "Type I sum of squares are “sequential.”  In essence the factors are tested in the order they are listed in the model. \n",
    "Type III sum of squares are “partial.”  In essence, every term in the model is tested in light of every other term in the model.  That means that main effects are tested in light of interaction terms as well as in light of other main effects. \n",
    "Type II sum of squares are similar to Type III, except that they preserve the principle of marginality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306abe2d",
   "metadata": {},
   "source": [
    "### Sol 10  Explain the concept of deviance in a GLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb47cc6",
   "metadata": {},
   "source": [
    "Deviance denotes the generalisation of RSS(Residual Sum of Squares) or difference between Actual and Predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b4c4e",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc3e7d",
   "metadata": {},
   "source": [
    "### Sol 11 What is regression analysis and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9252d41",
   "metadata": {},
   "source": [
    "Regression analyses the relationship between two or more features. \n",
    "The benefits of using Regression analysis are as follows:\n",
    "\n",
    "1. shows the significant relationships between the Lable (dependent variable) and the features(independent variable).\n",
    "2. It shows the extent of the impact of multiple independent variables on the dependent variable.\n",
    "3. It can also measure these effects even if the variables are on a different scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e9c0d",
   "metadata": {},
   "source": [
    "### Sol 12  What is the difference between simple linear regression and multiple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc36c39f",
   "metadata": {},
   "source": [
    "Simple Linear Regression contains one independentt variable while Multiple Regression contains one or more independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb8d58e",
   "metadata": {},
   "source": [
    "### Sol 13 How do you interpret the R-squared value in regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d90712",
   "metadata": {},
   "source": [
    "R2 value denote the proportion of variance explained by Linear Model but a high value of R2 always does not indicate a good fit and further investigation need to be done before coming to aconclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f4898",
   "metadata": {},
   "source": [
    "### Sol 14 What is the difference between correlation and regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec10d7",
   "metadata": {},
   "source": [
    "Corelation defines the strength of linear relationship between two variables(not Causation) whereas Regression analyses the relationship between two variables to make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350d287f",
   "metadata": {},
   "source": [
    "### Sol 15 What is the difference between the coefficients and the intercept in regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b351a7",
   "metadata": {},
   "source": [
    "Coefficent denotes the change in Predictor variable by unit change in Coefficient whereas Intercept denotes a constant term for Relation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c750118",
   "metadata": {},
   "source": [
    "### Sol 16  How do you handle outliers in regression analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a394c74",
   "metadata": {},
   "source": [
    "Outliers are detected by the following methods \n",
    "\n",
    "1. Z score Standardization \n",
    "2. Use of Median or mode as Decision Parameter (mean is influnced by Outlier)\n",
    "3. Box plot\n",
    "4. Scatter Plot\n",
    "\n",
    "They are generally removed or mapped to the reasonable value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac7ed6",
   "metadata": {},
   "source": [
    "### Sol 17  What is the difference between ridge regression and ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559342f",
   "metadata": {},
   "source": [
    "Ridge Regression includes a penalty(L2) in objective function of model to penalize the features not contributing to the Predictor variable causing generalization\n",
    "whereas in OLS the Loss function tends to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ec57e",
   "metadata": {},
   "source": [
    "### Sol 18 What is heteroscedasticity in regression and how does it affect the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5f4161",
   "metadata": {},
   "source": [
    "Heteroscedasity denotes a condition when error terms is not normally distributed or is corelated \n",
    "\n",
    "This cause problem and needs to be treated because it will lead to underestimation of pvalue of coefficients including the features which are not related but included in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11f5db",
   "metadata": {},
   "source": [
    "### Sol 19 How do you handle multicollinearity in regression analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02e55f0",
   "metadata": {},
   "source": [
    "Multicolinearity is detected by Variance Inflation factor and denotes correlation between independent variable and needs to be handled\n",
    "it is handled by \n",
    "1. Making a new variable by interaction of new variables\n",
    "2. Removing one of the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c5e68",
   "metadata": {},
   "source": [
    "### Sol 20 What is polynomial regression and when is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e17b6b",
   "metadata": {},
   "source": [
    "Sometimes the relation between predictor and indepent variable cannot be represented by linear term \n",
    "in that case we create a new variable with polynomial x^n where n is order \n",
    "\n",
    "althought the linear Regression method still can be used as the relation between predictor and coefficient is linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da2a849",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1b15ee",
   "metadata": {},
   "source": [
    "### Sol 21 What is a loss function and what is its purpose in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03524fe5",
   "metadata": {},
   "source": [
    "Loss function denotes the way to evaluate the performance of Machine learning model\n",
    "\n",
    "The Pupose is to minimize the Machine learning loss function for accurate predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60106a93",
   "metadata": {},
   "source": [
    "### Sol 22 What is the difference between a convex and non-convex loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858d289",
   "metadata": {},
   "source": [
    "A convex function is an important assumption in ML Algroithms as it generates global minima helping to find optimal solution \n",
    "Whereas in Non Convex function generates local Minima so it is difficult to find optimal Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa42de7",
   "metadata": {},
   "source": [
    "### Sol 23 What is mean squared error (MSE) and how is it calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7406207a",
   "metadata": {},
   "source": [
    "MSE denotes the square of difference between actual and predicted values divided by the number of observations and is loss function used commonly in OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926417c9",
   "metadata": {},
   "source": [
    "### Sol 24 What is mean absolute error (MAE) and how is it calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3370a97",
   "metadata": {},
   "source": [
    "MSE denotes the absolute difference between actual and predicted values divided by the number of observations and can be used as a Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d7b6d",
   "metadata": {},
   "source": [
    "### Sol 25 What is log loss (cross-entropy loss) and how is it calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48df03b0",
   "metadata": {},
   "source": [
    "A loss function that represents how much the predicted probabilities deviate from the true ones. It is used in binary cases. \n",
    "Cross-Entropy Loss: A generalized form of the log loss, which is used for multi-class classification problems.\n",
    "\n",
    "It is calculated by penalizing the missclassification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24589113",
   "metadata": {},
   "source": [
    "### Sol 26 How do you choose the appropriate loss function for a given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f9d043",
   "metadata": {},
   "source": [
    "Depends on type of target task \n",
    "\n",
    "1. Classification or Prediction \n",
    "2. Type of Metric to be measured \n",
    "3. he percentage of outliers in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290f9355",
   "metadata": {},
   "source": [
    "### Sol 27 Explain the concept of regularization in the context of loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddad925",
   "metadata": {},
   "source": [
    "Regularization includes adding a penalty term to loss functions to reduce overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1beb247d",
   "metadata": {},
   "source": [
    "### Sol 28 What is Huber loss and how does it handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483aded",
   "metadata": {},
   "source": [
    "Huber loss is a particular loss function (first introduced in 1964 by Peter Jost Huber, a Swiss mathematician) that is used widely for robust regression problems — situations where outliers are present that can degrade the performance and accuracy of least-squared-loss error based regression. \n",
    "\n",
    "It lies between absolute loss and squared loss it introduces a hyperparameter whhich is tuned in case of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f4d79",
   "metadata": {},
   "source": [
    "### Sol 29 What is quantile loss and when is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e73333",
   "metadata": {},
   "source": [
    "The value of quantile loss depends on whether a prediction is less or greater than the true value\n",
    "the quantile loss penalizes under-estimated predictions 4 times more than over-estimated. This way the model will be more critical to under-estimated errors and will predict higher values more often."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303d8cc",
   "metadata": {},
   "source": [
    "### Sol 30 What is the difference between squared loss and absolute loss?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae47440",
   "metadata": {},
   "source": [
    "Sol 30 Squared Loss denotes the square of deviation between predicted and actual value whereas absolute loss is mod "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf1de90",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02f10f",
   "metadata": {},
   "source": [
    "### Sol 31 What is an optimizer and what is its purpose in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6320fc",
   "metadata": {},
   "source": [
    "An optimizer is an algorithm or function that adapts the neural network's attributes, like learning rate and weights. it assists in improving the accuracy and reduces the total loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa062ef",
   "metadata": {},
   "source": [
    "### Sol 32 What is Gradient Descent (GD) and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8f8ef",
   "metadata": {},
   "source": [
    "Gradient descent is a generic optimization algorithm capable of finding optimal solu‐\n",
    "tions to a wide range of problems.\n",
    "It works by updating the parameters of model continuosly till we find an optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8fc2d9",
   "metadata": {},
   "source": [
    "### Sol 33 What are the different variations of Gradient Descent?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4c17b61",
   "metadata": {},
   "source": [
    "1.Batch Gradient Descent \n",
    "2. Minibatch Gradient Descent \n",
    "3. Stochastic Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3878d6e",
   "metadata": {},
   "source": [
    "### Sol 34 What is the learning rate in GD and how do you choose an appropriate value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b6fc1",
   "metadata": {},
   "source": [
    "Learning rate denotes the value by which the parameters will be updated in Gradient Descent \n",
    "Its appropraiate value is chosen by Hyperparameter Tuning and implmented by Grid Search/Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4874db",
   "metadata": {},
   "source": [
    "### Sol 35 How does GD handle local optima in optimization problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1526f",
   "metadata": {},
   "source": [
    "The MSE cost function for a linear regression model happens to be a convex function, which means that if we pick any two points on the curve, the line\n",
    "segment joining them is never below the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that\n",
    "never changes abruptly.2 These two facts have a great consequence: gradient descent is guaranteed to approach arbitrarily closely the global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008af70d",
   "metadata": {},
   "source": [
    "### Sol 36 What is Stochastic Gradient Descent (SGD) and how does it differ from GD?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01ca716",
   "metadata": {},
   "source": [
    "stochastic gradient descent picks arandom instance in the training set at every step and computes the gradients based\n",
    "only on that single instance(####GD picks complete instance at once). Obviously, working on a single instance at a time makes the algorithm much faster because it has very little data to manipulate at every\n",
    "iteration. It also makes it possible to train on huge training sets, since only one\n",
    "instance needs to be in memory at each iteration\n",
    "\n",
    "this algorithm is much less regular than batch gradient descent: instead of gently decreasing until it\n",
    "reaches the minimum, the cost function will bounce up and down, decreasing only\n",
    "on average. Over time it will end up very close to the minimum, but once it gets there\n",
    "it will continue to bounce around, never settling down. Once the\n",
    "algorithm stops, the final parameter values will be good, but not optimal.\n",
    "\n",
    "this can be handled by tweaking the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905324e",
   "metadata": {},
   "source": [
    "### Sol 37 Explain the concept of batch size in GD and its impact on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46100951",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch size has no such large  Impact on the GD except that Large batch require no scaling but training is slow\n",
    "all these algorithms end up with very similar models and make predictions in exactly the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7933e",
   "metadata": {},
   "source": [
    "### Sol 38  What is the role of momentum in optimization algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ba5447",
   "metadata": {},
   "source": [
    "Momentum is an extension to the gradient descent optimization algorithm that allows the search to build inertia in a direction in the search space and overcome the oscillations of noisy gradients and coast across flat spots of the search space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05513be5",
   "metadata": {},
   "source": [
    "### Sol 39 What is the difference between batch GD, mini-batch GD, and SGD?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc762e7",
   "metadata": {},
   "source": [
    "Already explained in above question 35 and 36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b31466",
   "metadata": {},
   "source": [
    "### Sol 40 How does the learning rate affect the convergence of GD?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27740070",
   "metadata": {},
   "source": [
    "A Decent value of learning rate is neccesary because a small value will take two much time to train and reach minima \n",
    "whereas a large value will jump around the minima value \n",
    "\n",
    "The value is selected by Hyperparmeter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e60b58",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596a5091",
   "metadata": {},
   "source": [
    "###  Sol 41 What is regularization and why is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948060ed",
   "metadata": {},
   "source": [
    "Regularization includes a penalty term in objective function of model to penalize the features not contributing to the Predictor variable causing generalization and redducing Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2a485e",
   "metadata": {},
   "source": [
    "### Sol 42 What is the difference between L1 and L2 regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeaafa1",
   "metadata": {},
   "source": [
    " L1 and L2 differs in the norm and type of penalty they put on features that are not realted to the predictor variable\n",
    " L1 also known as Lasso also helps in feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630f6f7",
   "metadata": {},
   "source": [
    "### Sol 43  Explain the concept of ridge regression and its role in regularization."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3e88e7e",
   "metadata": {},
   "source": [
    "It is technique of Regularization also Known as L2 Regularization \n",
    "It works as\n",
    "1.penalizes the size  magnitude of the regression coefficients by adding a squad term \n",
    "2.enforces the coefficients to be lower, but not 0\n",
    "3.minimizes irrelevant features and does not remove them  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff947d9b",
   "metadata": {},
   "source": [
    "### Sol 44 What is the elastic net regularization and how does it combine L1 and L2 penalties?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053551a6",
   "metadata": {},
   "source": [
    "Elastic Net (L1+L2 Regularization)\n",
    "\n",
    "penalizes the size  magnitude of the regression and  absolute value of the coefficients\n",
    "sets irrelevant features to 0 and enforces the coefficients to be lower thus combining properties of L1 and l2 \n",
    "\n",
    "It has a hyperparameter lamda varying from 0 to 1 which decides the weightage of L1 and l2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e6eca",
   "metadata": {},
   "source": [
    "### Sol 45  How does regularization help prevent overfitting in machine learning models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24bb043",
   "metadata": {},
   "source": [
    "Regularization includes a penalty term in objective function of model to penalize the features not contributing to the Predictor variable causing generalization and redducing Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1a8a23",
   "metadata": {},
   "source": [
    "### Sol 46 What is early stopping and how does it relate to regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609123d",
   "metadata": {},
   "source": [
    "Early stopping is a popular regularization technique due to its simplicity and effectiveness. \n",
    "\n",
    "In early stopping, the algorithm is trained using the training set, and the point at which to stop training is determined from the validation set. Training error and validation error are analyzed. The training error steadily decreases while the validation error decreases until a point, after which it increases. This is because, during training, the learning model starts to overfit the training data. This causes the training error to decrease while the validation error increases. So a model with better validation set error can be obtained if the parameters that give the least validation set error are used. Each time the error on the validation set decreases, a copy of the model parameters is stored. When the training algorithm terminates, these parameters which give the least validation set error are finally returned and not the last modified parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51e8af",
   "metadata": {},
   "source": [
    "### Sol 47 Explain the concept of dropout regularization in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b375f08",
   "metadata": {},
   "source": [
    "Dropout is a simple and powerful regularization technique for neural networks and deep learning models.\n",
    "\n",
    "Dropout Regularization for Neural Networks\n",
    "Dropout is a regularization technique for neural network models proposed by Srivastava et al. in their 2014 paper “Dropout: A Simple Way to Prevent Neural Networks from Overfitting” (download the PDF).\n",
    "\n",
    "Dropout is a technique where randomly selected neurons are ignored during training. They are “dropped out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass, and any weight updates are not applied to the neuron on the backward pass.\n",
    "\n",
    "As a neural network learns, neuron weights settle into their context within the network. Weights of neurons are tuned for specific features, providing some specialization. Neighboring neurons come to rely on this specialization, which, if taken too far, can result in a fragile model too specialized for the training data. This reliance on context for a neuron during training is referred to as complex co-adaptations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38685650",
   "metadata": {},
   "source": [
    "### Sol 48 How do you choose the regularization parameter in a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea9a92",
   "metadata": {},
   "source": [
    "Start with initial guess and then Hyperparmater Tuning to select the best parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d2ef6c",
   "metadata": {},
   "source": [
    "### Sol 49 What is the difference between feature selection and regularization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d9e3e",
   "metadata": {},
   "source": [
    "Feature selection means selecting the set of features which are capturing most of the desired variance of the Data\n",
    "Whereas Regularization controls overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24dcdf6",
   "metadata": {},
   "source": [
    "### Sol 50 What is the trade-off between bias and variance in regularized models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ede86f",
   "metadata": {},
   "source": [
    "Bias-variance tradeoff decomposes the model's Mean Square Error into two parts: Bias and Variance.\n",
    "\n",
    "Bias- is a measure of how near the model is to the actual function we are trying to model. If your model has a high bias, the model is underfitting; this means you will do poorly on the training and test data, but the relative results will be similar. \n",
    "\n",
    "Ways to improve bias include making the model more complex, adding higher order polynomials, obtaining more features or finding more data.\n",
    "\n",
    "\n",
    "Variance - is the average squared difference of each model you train relative to the average prediction of each model. If your model has high variance, the model will usually overfit the data; this means you will do well on the training data but not on the testing data. \n",
    "\n",
    "\n",
    "We can improve variance by making the model less complex, i.e., lowering the order of the polynomial, obtaining more data  or using Reguliztion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84281abc",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8307b7a4",
   "metadata": {},
   "source": [
    "### Sol 51 What is Support Vector Machines (SVM) and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edac965",
   "metadata": {},
   "source": [
    "Support Vector Machine is a supervised Machine Learning algorithm widely used for solving different machine learning problems. Given a dataset, the algorithm tries to divide the data using hyperplanes and then makes the predictions. SVM is a non-probabilistic linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b338e4",
   "metadata": {},
   "source": [
    "### Sol 52 How does the kernel trick work in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa771469",
   "metadata": {},
   "source": [
    "Kernel Tricks uses kernel function to compute in a higher-dimensional space without calculating the new coordinates in that higher dimension. It implicitly uses predefined mathematical functions to do operations on the existing points which mimic the computation in a higher-dimensional space without adding to the computation cost as they are not actually calculating the coordinates in the higher dimension thereby avoiding the computation of calculating distances from the newly computed points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d31493",
   "metadata": {},
   "source": [
    "### Sol 53 What are support vectors in SVM and why are they important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e83df4",
   "metadata": {},
   "source": [
    "Support Vectors denote the data points lying closer to the marginal planes and define its decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e47d3a",
   "metadata": {},
   "source": [
    "### Sol 54 Explain the concept of the margin in SVM and its impact on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b52733d",
   "metadata": {},
   "source": [
    "Margin is kept in SVM is defined as the distance between the decision boundaries and decides the classification/Regression performance \n",
    "\n",
    "Eg A Hyperparameter C decides the classification error and decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ef2d23",
   "metadata": {},
   "source": [
    "### Sol 55 How do you handle unbalanced datasets in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767b485",
   "metadata": {},
   "source": [
    "SVMs work fine on sparse and unbalanced data. Class-weighted SVM is designed to deal with unbalanced data by assigning higher misclassification penalties to training instances of the minority class.\n",
    "\n",
    "THis parameter can be setted during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4be8c",
   "metadata": {},
   "source": [
    "### Sol 56 What is the difference between linear SVM and non-linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca69764",
   "metadata": {},
   "source": [
    "Linearly Seprable denotes the condition when data points can be classified by a linear Decision Boundary \n",
    "Wherals non linear cannot be classified directly by Hyperplane \n",
    "We use Kernels to make non-separable data into separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d007c15a",
   "metadata": {},
   "source": [
    "### Sol 57 What is the role of C-parameter in SVM and how does it affect the decision boundary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e6a65",
   "metadata": {},
   "source": [
    "C is essentially a regularization parameter, it trades off between the misclassification of training examples against simplicity of the decision surface at the cost of Misclassification . A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea1759",
   "metadata": {},
   "source": [
    "### Sol 59 What is the difference between hard margin and soft margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ecf6e",
   "metadata": {},
   "source": [
    "Hard Margin means no missclassification whereas Soft margin means the errors are taken in loss function(Hinge loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8a9f1",
   "metadata": {},
   "source": [
    "### Sol 60 How do you interpret the coefficients in an SVM model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f408a0b",
   "metadata": {},
   "source": [
    "Weight vectors denote the orientation of lines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d09ba3",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aabf9b",
   "metadata": {},
   "source": [
    "### Sol 61 What is a decision tree and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b658f3",
   "metadata": {},
   "source": [
    "Decision Tree is a popular ml model and it helps giving the rule set by construction of trees and nodes \n",
    "\n",
    "The node at the top consist of splitting criteria whereas the node at the bottom contains decision rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46201b",
   "metadata": {},
   "source": [
    "### Sol 62 How do you make splits in a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbfd79a",
   "metadata": {},
   "source": [
    "Splits are made on purity of node ie the split which gives maximum Information decides the split criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3146bb3",
   "metadata": {},
   "source": [
    "### Sol 63 What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e629cb3",
   "metadata": {},
   "source": [
    "Impurity Measures are used to decide the split criteria \n",
    "\n",
    "Entropy is the measure of randomness in the data. In other words, it gives the impurity present in the dataset.\n",
    "We aim to minimise the entropy \n",
    "\n",
    "Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset Just like entropy we minimise the Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571cf248",
   "metadata": {},
   "source": [
    "### Sol 64 Explain the concept of information gain in decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0248b524",
   "metadata": {},
   "source": [
    "Information gain calculates the decrease in entropy after splitting a node. It is the difference between entropies before and after the split. The more the information gain, the more entropy is remo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cdaacf",
   "metadata": {},
   "source": [
    "### Sol 65  How do you handle missing values in decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba16e99",
   "metadata": {},
   "source": [
    "Some older algroithms like ID3neglect missing value while some other treat it as a seprate category \n",
    "all in all this is not influenced by missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6766c0",
   "metadata": {},
   "source": [
    "### Sol 66 What is pruning in decision trees and why is it important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91921c5c",
   "metadata": {},
   "source": [
    "Tree pruning is the method of trimming down a full tree (obtained through the above process) to reduce the complexity and variance in the data. \n",
    "\n",
    "The Types of Pruning are \n",
    "\n",
    "Post-pruning\n",
    "Post-pruning, also known as backward pruning, is the process where the decision tree is generated first and then the non-significant branches are removed. Cross-validation set of data is used to check the effect of pruning and tests whether expanding a node will make an improvement or not. If any improvement is there then we continue by expanding that node else if there is reduction in accuracy then the node not be expanded and should be converted in a leaf node.\n",
    "\n",
    "Pre-pruning\n",
    "Pre-pruning, also known as forward pruning, stops the non-significant branches from generating. It uses a condition to decide when should it terminate splitting of some of the branches prematurely as the tree is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94b9ffd",
   "metadata": {},
   "source": [
    "### Sol 67 What is the difference between a classification tree and a regression tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1a2318",
   "metadata": {},
   "source": [
    "In a classification problem the criteria is to decide a specific category whereas in Regression we map the mean of the element"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea2a37",
   "metadata": {},
   "source": [
    "### Sol 68 How do you interpret the decision boundaries in a decision tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004803de",
   "metadata": {},
   "source": [
    "Decision Boundary helps to map the independent feature for respective class based on tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1781c7d0",
   "metadata": {},
   "source": [
    "### Sol 69 What is the role of feature importance in decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e9fd3",
   "metadata": {},
   "source": [
    "feature_importances_ that helps us understand which features are actually helpful compared to others.\n",
    "\n",
    "The weighted impurity decrease equation is the following:\n",
    "\n",
    "N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                      - N_t_L / N_t * left_impurity)\n",
    "                      \n",
    "Weightage of impurity denotes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff125dfe",
   "metadata": {},
   "source": [
    "### Sol 70 What are ensemble techniques and how are they related to decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ffc23e",
   "metadata": {},
   "source": [
    "Ensemble Techniques work on the principle of combining various weaker models to constitute a stronger model \n",
    "\n",
    "(Majority Voting in case of Classification while Mean in case of Regression)\n",
    "\n",
    "Most of the ensemble techniques utilize decision trees as base and combines the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8bddb2",
   "metadata": {},
   "source": [
    "## Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76142c1",
   "metadata": {},
   "source": [
    "### Sol 71  What are ensemble techniques in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d4a39",
   "metadata": {},
   "source": [
    "Ensemble methods take multiple small models instead of single model and combine their predictions to obtain a more powerful predictive power.\n",
    "\n",
    "Any model can be taken as a base model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e1b87",
   "metadata": {},
   "source": [
    "### Sol 72 What is bagging and how is it used in ensemble learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49335b3f",
   "metadata": {},
   "source": [
    "Bagging is the type of ensemble technique in which a single training algorithm is used on different subsets of the training data where the subset sampling is done with replacement (bootstrap). Once the algorithm is trained on all the subsets, then bagging makes the prediction by aggregating all the predictions made by the algorithm on different subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed02d803",
   "metadata": {},
   "source": [
    "### Sol 73  Explain the concept of bootstrapping in bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f01ad",
   "metadata": {},
   "source": [
    "Bootstrapping is a technique of sampling different sets of data from a given training set by using replacement. After bootstrapping the training dataset, we train model on all the different sets and aggregate the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c112fd0",
   "metadata": {},
   "source": [
    "### Sol 74 What is boosting and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da469b1",
   "metadata": {},
   "source": [
    "Boosting is an ensemble approach(meaning it involves several trees) that starts from a weaker decision and keeps on building the models such that the final prediction is the weighted sum of all the weaker decision-makers.\n",
    "The weights are assigned based on the performance of an individual tree.\n",
    "Ensemble parameters are calculated in **stagewise way** which means that while calculating the subsequent weight, the learning from the previous tree is considered as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4977d5",
   "metadata": {},
   "source": [
    "### Sol 75  What is the difference between AdaBoost and Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323292f",
   "metadata": {},
   "source": [
    "Adaboosting Techniques work on assigining the increase of weightage of missclassified points and reducing the weightage of correctly classified points\n",
    "\n",
    "Gradient Boosting work on working on the residuals instead of orignal points after stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d079ed7f",
   "metadata": {},
   "source": [
    "### Sol 76 What is the purpose of random forests in ensemble learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22bf1a2",
   "metadata": {},
   "source": [
    "Random forest which is more convenient and well optimized for decision trees. The main issue with bagging is that there is not much independence among the sampled datasets i.e. there is correlation. The advantage of random forests over bagging models is that the random forests makes a tweak in the working algorithm of bagging model to decrease the correlation in trees. The idea is to introduce more randomness while creating trees which will help in reducing correlation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7b881",
   "metadata": {},
   "source": [
    "### Sol 77 How do random forests handle feature importance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b420df27",
   "metadata": {},
   "source": [
    "In bootstraping we allow all the sample data to be used for splitting the nodes but not with random forests. When building a decision tree, each time a split is to happen, a random sample of ‘m’ predictors are chosen from the total ‘p’ predictors. Only those ‘m’ predictors are allowed to be used for the split.\n",
    "\n",
    "This is because  in those ‘p’ predictors, 1 predictor is very strong. Now each sample this predictor will remain the strongest. So, whenever trees will be built for these sampled data, this predictor will be chosen by all the trees for splitting and thus will result in similar kind of tree formation for each bootstrap model. This introduces correaltion in the dataset and averaging correalted dataset results do not lead low variance. That’s why in random forest the choice for selecting node for split is limited and it introduces randomness in the formation of the trees as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15362cc",
   "metadata": {},
   "source": [
    "### Sol 78 What is stacking in ensemble learning and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf32c6b",
   "metadata": {},
   "source": [
    "Stacking takes several classification models called base learners and uses their output as the input for the meta-classifier. \n",
    "\n",
    "So instead of directly training base data they take the input as the base models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95137b0d",
   "metadata": {},
   "source": [
    "### Sol 79 What are the advantages and disadvantages of ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131db19",
   "metadata": {},
   "source": [
    "Advantages\n",
    "\n",
    "1. Accurate Prediction Models\n",
    "2. Stable and Robust Predictions\n",
    "\n",
    "Disadvantages \n",
    "\n",
    "1. ensembles cost more to create, train, and deploy. \n",
    "2. Difficult to intrepret\n",
    "3. ensembles cannot help unknown differences between sample and population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6a90f",
   "metadata": {},
   "source": [
    "### Sol 80 How do you choose the optimal number of models in an ensemble?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84065e23",
   "metadata": {},
   "source": [
    "We can start with a initial emprical relation and then can decide the number of models based on target metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19151e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
